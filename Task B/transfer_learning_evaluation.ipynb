{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transfer learning evaluation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOPrDRw/CIW30v7/vl/xany",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tobyloby12/AMLS-assignment/blob/main/Task%20B/transfer_learning_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPRppndUmj7T",
        "outputId": "253bfea6-50e2-4143-842c-e93790b5a664"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# mounting google drive (not needed if not on google colab)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# put correct path to AMLSassignment github folder\n",
        "path = '/content/drive/MyDrive/amlsAssignment'"
      ],
      "metadata": {
        "id": "WToFyR3YOLF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(path)"
      ],
      "metadata": {
        "id": "LxMke9eFmqm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os"
      ],
      "metadata": {
        "id": "qQgxf1Qwmv_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using the image_dataset_from_directory function to create the datasets for training of neural networks from the directory created above\n",
        "\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "os.chdir(path)\n",
        "train_dataset = image_dataset_from_directory(\n",
        "    os.getcwd() + '/dataset/class_dataset',\n",
        "    validation_split=0.3,\n",
        "    subset='training',\n",
        "    seed=0,\n",
        "    image_size=(256, 256),\n",
        "    batch_size=32\n",
        "\n",
        ")\n",
        "\n",
        "validate_dataset = image_dataset_from_directory(\n",
        "    os.getcwd() + '/dataset/class_dataset',\n",
        "    validation_split=0.3,\n",
        "    subset='validation',\n",
        "    seed=0,\n",
        "    image_size=(256, 256),\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "test_dataset = image_dataset_from_directory(\n",
        "    os.getcwd() + '/dataset/class_dataset_test',\n",
        "    seed=0,\n",
        "    image_size=(256, 256),\n",
        "    batch_size=32\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtIWdHGpmxpl",
        "outputId": "b6a2f907-0e34-4245-e95c-429c8e30ef98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3000 files belonging to 4 classes.\n",
            "Using 2100 files for training.\n",
            "Found 3000 files belonging to 4 classes.\n",
            "Using 900 files for validation.\n",
            "Found 200 files belonging to 4 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# preparing datasets like in the transfer learning augmented notebook\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "IMG_SIZE = 256\n",
        "\n",
        "resize_and_rescale = tf.keras.Sequential([\n",
        "  layers.Resizing(IMG_SIZE, IMG_SIZE),\n",
        "  layers.Rescaling(1./255)\n",
        "])\n",
        "\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "  layers.RandomFlip(\"horizontal\"),\n",
        "  layers.RandomRotation(0.5),\n",
        "])\n",
        "\n",
        "batch_size = 32\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "def prepare(ds, shuffle=False, augment=False):\n",
        "  # Resize and rescale all datasets.\n",
        "  ds = ds.map(lambda x, y: (resize_and_rescale(x), y), \n",
        "              num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(1000)\n",
        "\n",
        "  # Batch all datasets.\n",
        "  # ds = ds.batch(batch_size)\n",
        "\n",
        "  # Use data augmentation only on the training set.\n",
        "  if augment:\n",
        "    ds = ds.map(lambda x, y: (data_augmentation(x, training=True), y), \n",
        "                num_parallel_calls=AUTOTUNE)\n",
        "    # ds = ds.map(lambda x, y: (data_augmentation(x, training=True), y), \n",
        "    #             num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "\n",
        "  # Use buffered prefetching on all datasets.\n",
        "  return ds.prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "ds_train = prepare(train_dataset, augment=True, shuffle=True)\n",
        "ds_validate = prepare(validate_dataset)\n",
        "ds_test = prepare(test_dataset)"
      ],
      "metadata": {
        "id": "FQLK18qX6U3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating function that will produce a classification report and append it to a dataframe so that the model accuracies and f1 scores can be comnpared\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import classification_report\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.layers import Flatten, Dense\n",
        "from tensorflow.keras import models\n",
        "def evaluate(model_name, df, model_weights, augment=False):\n",
        "  # if function to choose whether data is augmented or not\n",
        "  if augment == False:\n",
        "    data = test_dataset\n",
        "  else:\n",
        "    data = ds_test\n",
        "  # if else clause to choose what model is being used\n",
        "  if model_name == 'VGG16':\n",
        "    model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(256, 256, 3))\n",
        "    model = models.Sequential([\n",
        "      model,\n",
        "      Flatten(),\n",
        "      Dense(1024, activation='relu'),\n",
        "      Dense(4, activation='softmax')\n",
        "    ])\n",
        "\n",
        "\n",
        "  elif model_name == 'Xception':\n",
        "      xception_model = tf.keras.applications.Xception(\n",
        "          include_top=False,\n",
        "          weights=\"imagenet\",\n",
        "          input_tensor=None,\n",
        "          input_shape=(256, 256, 3),\n",
        "          pooling=None\n",
        "      )\n",
        "      model = models.Sequential([\n",
        "          xception_model,\n",
        "          Flatten(),\n",
        "          Dense(1024, activation='relu'),\n",
        "          Dense(4, activation='softmax')\n",
        "      ])\n",
        "\n",
        "\n",
        "  elif model_name == 'efficientNet':\n",
        "    efficient_model = tf.keras.applications.EfficientNetB4(\n",
        "        include_top=False,\n",
        "        weights=\"imagenet\",\n",
        "        input_tensor=None,\n",
        "        input_shape=(256, 256, 3),\n",
        "        pooling=True,\n",
        "        classes=1000,\n",
        "        classifier_activation=\"softmax\"\n",
        "    )\n",
        "    model = models.Sequential([\n",
        "          efficient_model,\n",
        "          Flatten(),\n",
        "          Dense(10, activation='relu'),\n",
        "          Dense(4, activation='softmax')\n",
        "    ])\n",
        "\n",
        "  # loading model and making predictions\n",
        "  model.load_weights(model_weights)\n",
        "  predictions = np.array([])\n",
        "  labels =  np.array([])\n",
        "  for x, y in data:\n",
        "    predictions = np.concatenate([predictions, np.argmax(model.predict(x), axis = -1)])\n",
        "    labels = np.concatenate([labels, y])\n",
        "\n",
        "  # getting classification report to append to dataframe\n",
        "  report = classification_report(\n",
        "      labels,\n",
        "      predictions, \n",
        "      output_dict=True, \n",
        "      target_names=['meningioma_tumor', \n",
        "                    'no_tumor', \n",
        "                    'glioma_tumor', \n",
        "                    'pituitary_tumor'])\n",
        "  \n",
        "  row = {\n",
        "      'Architecture': model_name,\n",
        "      'Accuracy': \"{:.4f}\".format(report['accuracy']), \n",
        "      'F1 meningioma': \"{:.4f}\".format(report['meningioma_tumor']['f1-score']), \n",
        "      'F1 no_tumor': \"{:.4f}\".format(report['no_tumor']['f1-score']), \n",
        "      'F1 glioma_tumor': \"{:.4f}\".format(report['glioma_tumor']['f1-score']), \n",
        "      'F1 pituitary_tumor': \"{:.4f}\".format(report['pituitary_tumor']['f1-score'])\n",
        "  }\n",
        "  # appending the row to datafrome with all the information\n",
        "  df = df.append(row, ignore_index=True)\n",
        "    \n",
        "  return df"
      ],
      "metadata": {
        "id": "QQEjKpj2bu5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating initial dataframe\n",
        "df = pd.DataFrame(columns=['Architecture', 'Accuracy', 'F1 meningioma', 'F1 no_tumor', 'F1 glioma_tumor', 'F1 pituitary_tumor'])"
      ],
      "metadata": {
        "id": "mhhnC0oHdZEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluating all models\n",
        "df = evaluate('VGG16', df, 'best_model_VGG16_3.h5')\n",
        "df = evaluate('Xception', df, 'best_model_Xception_1.h5')\n",
        "df = evaluate('efficientNet', df, 'best_model_efficient_4.h5')\n",
        "df = evaluate('VGG16', df, 'best_model_VGG16_augment_1.h5', augment=True)\n",
        "df = evaluate('Xception', df, 'best_model_Xception_augmented_1.h5', augment=True)\n",
        "df = evaluate('efficientNet', df, 'best_model_efficient_augmented_2.h5', augment=True)"
      ],
      "metadata": {
        "id": "nFDOivYGdd6Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dd1a28c-f9c2-4d44-a41f-363d1d355ca8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n",
            "58900480/58889256 [==============================] - 1s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "83689472/83683744 [==============================] - 0s 0us/step\n",
            "83697664/83683744 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb4_notop.h5\n",
            "71688192/71686520 [==============================] - 1s 0us/step\n",
            "71696384/71686520 [==============================] - 1s 0us/step\n",
            "WARNING:tensorflow:5 out of the last 15 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f7303bc5440> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# saving dataframe\n",
        "df.to_csv('results_CNN.csv')"
      ],
      "metadata": {
        "id": "907Yw-9J3Hrn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}