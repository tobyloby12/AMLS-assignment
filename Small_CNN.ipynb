{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Small CNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMxdOAqHpPNnrN2Z7r5d9iN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tobyloby12/AMLS-assignment/blob/main/Small_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2UbsD7fdyOp",
        "outputId": "cf60a69f-f338-4197-c757-7a20de1f5a17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/gdrive/MyDrive/amlsAssignment')"
      ],
      "metadata": {
        "id": "x4kgWk2nd9hy"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, Dense, MaxPool2D, Dropout, Softmax, Flatten, Rescaling\n",
        "from tensorflow.keras.initializers import GlorotNormal\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Rescaling(1./255),)\n",
        "\n",
        "model.add(Conv2D(filters=16, kernel_size=5, padding= 'same', activation='relu', kernel_initializer=GlorotNormal))\n",
        "model.add(Dropout(.5))\n",
        "model.add(MaxPool2D(pool_size= 2, strides= 2, padding= 'same'))\n",
        "\n",
        "model.add(Conv2D(filters=32, kernel_size=3, padding= 'same', activation='relu'))\n",
        "model.add(Dropout(.5))\n",
        "model.add(MaxPool2D(pool_size= 2, strides= 2, padding= 'same'))\n",
        "\n",
        "model.add(Conv2D(filters=64, kernel_size=3, padding= 'same', activation='relu'))\n",
        "model.add(Dropout(.5))\n",
        "model.add(MaxPool2D(pool_size= 2, strides= 2, padding= 'same'))\n",
        "\n",
        "model.add(Conv2D(filters=128, kernel_size=3, padding= 'same', activation='relu'))\n",
        "model.add(Dropout(.5))\n",
        "model.add(MaxPool2D(pool_size= 2, strides= 2, padding= 'same'))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(1024))\n",
        "model.add(Dense(4))"
      ],
      "metadata": {
        "id": "yAbVXCRse-iS"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "\n",
        "train_dataset = image_dataset_from_directory(\n",
        "    os.getcwd() + '/dataset/class_dataset',\n",
        "    validation_split=0.3,\n",
        "    subset='training',\n",
        "    seed=0,\n",
        "    image_size=(256, 256),\n",
        "    batch_size=32\n",
        "\n",
        ")\n",
        "\n",
        "validate_dataset = image_dataset_from_directory(\n",
        "    os.getcwd() + '/dataset/class_dataset',\n",
        "    validation_split=0.3,\n",
        "    subset='validation',\n",
        "    seed=0,\n",
        "    image_size=(256, 256),\n",
        "    batch_size=32\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6wW34aNjq7E",
        "outputId": "8c6e6b0b-0bec-4239-af94-0e9c95ca9349"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3000 files belonging to 4 classes.\n",
            "Using 2100 files for training.\n",
            "Found 3000 files belonging to 4 classes.\n",
            "Using 900 files for validation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for image_batch, labels_batch in train_dataset:\n",
        "  print(image_batch.shape)\n",
        "  print(labels_batch.shape)\n",
        "  break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Uqx39stWvKn",
        "outputId": "97ea70f3-3c1d-4499-9cd3-adb6e51245ff"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 256, 256, 3)\n",
            "(32,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "\n",
        "model.compile(optimizer='adam', loss=SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "qBST-6xblgfM"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_dataset, validation_data=validate_dataset, epochs = 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_Hx7Ghmnbie",
        "outputId": "497e5e7c-9d42-4178-e2e7-6f4d5b7ef27e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "66/66 [==============================] - 14s 195ms/step - loss: 0.5842 - accuracy: 0.7700 - val_loss: 1.5118 - val_accuracy: 0.3333\n",
            "Epoch 2/100\n",
            "66/66 [==============================] - 14s 195ms/step - loss: 0.4718 - accuracy: 0.8181 - val_loss: 1.5205 - val_accuracy: 0.3133\n",
            "Epoch 3/100\n",
            "66/66 [==============================] - 14s 195ms/step - loss: 0.4318 - accuracy: 0.8381 - val_loss: 1.5495 - val_accuracy: 0.2578\n",
            "Epoch 4/100\n",
            "66/66 [==============================] - 14s 196ms/step - loss: 0.3892 - accuracy: 0.8624 - val_loss: 1.4084 - val_accuracy: 0.3322\n",
            "Epoch 5/100\n",
            "66/66 [==============================] - 14s 196ms/step - loss: 0.3521 - accuracy: 0.8671 - val_loss: 1.5839 - val_accuracy: 0.3722\n",
            "Epoch 6/100\n",
            "66/66 [==============================] - 14s 194ms/step - loss: 0.2981 - accuracy: 0.8838 - val_loss: 1.5440 - val_accuracy: 0.2956\n",
            "Epoch 7/100\n",
            "66/66 [==============================] - 14s 194ms/step - loss: 0.3220 - accuracy: 0.8905 - val_loss: 1.4250 - val_accuracy: 0.2544\n",
            "Epoch 8/100\n",
            "66/66 [==============================] - 14s 195ms/step - loss: 0.2799 - accuracy: 0.8948 - val_loss: 1.6620 - val_accuracy: 0.2811\n",
            "Epoch 9/100\n",
            "66/66 [==============================] - 14s 194ms/step - loss: 0.2656 - accuracy: 0.8967 - val_loss: 1.5472 - val_accuracy: 0.3144\n",
            "Epoch 10/100\n",
            "66/66 [==============================] - 14s 196ms/step - loss: 0.2397 - accuracy: 0.9095 - val_loss: 1.6363 - val_accuracy: 0.2378\n",
            "Epoch 11/100\n",
            "66/66 [==============================] - 14s 194ms/step - loss: 0.2158 - accuracy: 0.9176 - val_loss: 1.2230 - val_accuracy: 0.4189\n",
            "Epoch 12/100\n",
            "66/66 [==============================] - 14s 196ms/step - loss: 0.2360 - accuracy: 0.9152 - val_loss: 1.9276 - val_accuracy: 0.3067\n",
            "Epoch 13/100\n",
            "66/66 [==============================] - 14s 194ms/step - loss: 0.2100 - accuracy: 0.9214 - val_loss: 1.7521 - val_accuracy: 0.2989\n",
            "Epoch 14/100\n",
            "66/66 [==============================] - 14s 193ms/step - loss: 0.1820 - accuracy: 0.9319 - val_loss: 1.5248 - val_accuracy: 0.2611\n",
            "Epoch 15/100\n",
            "66/66 [==============================] - 14s 194ms/step - loss: 0.2206 - accuracy: 0.9214 - val_loss: 1.8437 - val_accuracy: 0.1822\n",
            "Epoch 16/100\n",
            "66/66 [==============================] - 14s 195ms/step - loss: 0.1818 - accuracy: 0.9310 - val_loss: 1.2871 - val_accuracy: 0.4000\n",
            "Epoch 17/100\n",
            "66/66 [==============================] - 14s 196ms/step - loss: 0.1828 - accuracy: 0.9295 - val_loss: 1.3520 - val_accuracy: 0.3089\n",
            "Epoch 18/100\n",
            "66/66 [==============================] - 14s 194ms/step - loss: 0.1967 - accuracy: 0.9248 - val_loss: 1.3820 - val_accuracy: 0.4789\n",
            "Epoch 19/100\n",
            "66/66 [==============================] - 14s 197ms/step - loss: 0.1403 - accuracy: 0.9471 - val_loss: 1.2065 - val_accuracy: 0.4511\n",
            "Epoch 20/100\n",
            "66/66 [==============================] - 15s 222ms/step - loss: 0.1574 - accuracy: 0.9381 - val_loss: 1.5979 - val_accuracy: 0.3733\n",
            "Epoch 21/100\n",
            "66/66 [==============================] - 14s 196ms/step - loss: 0.1338 - accuracy: 0.9529 - val_loss: 2.2531 - val_accuracy: 0.1978\n",
            "Epoch 22/100\n",
            "66/66 [==============================] - 14s 196ms/step - loss: 0.1584 - accuracy: 0.9414 - val_loss: 2.0444 - val_accuracy: 0.1767\n",
            "Epoch 23/100\n",
            "66/66 [==============================] - 14s 194ms/step - loss: 0.1580 - accuracy: 0.9438 - val_loss: 1.1416 - val_accuracy: 0.4689\n",
            "Epoch 24/100\n",
            "66/66 [==============================] - 14s 196ms/step - loss: 0.1130 - accuracy: 0.9538 - val_loss: 1.4180 - val_accuracy: 0.4256\n",
            "Epoch 25/100\n",
            "66/66 [==============================] - 14s 196ms/step - loss: 0.1270 - accuracy: 0.9533 - val_loss: 1.7039 - val_accuracy: 0.2222\n",
            "Epoch 26/100\n",
            "66/66 [==============================] - 14s 196ms/step - loss: 0.1457 - accuracy: 0.9481 - val_loss: 2.2775 - val_accuracy: 0.2056\n",
            "Epoch 27/100\n",
            "66/66 [==============================] - 14s 197ms/step - loss: 0.1150 - accuracy: 0.9576 - val_loss: 1.6801 - val_accuracy: 0.2667\n",
            "Epoch 28/100\n",
            "66/66 [==============================] - 14s 197ms/step - loss: 0.1159 - accuracy: 0.9567 - val_loss: 1.7046 - val_accuracy: 0.3733\n",
            "Epoch 29/100\n",
            "66/66 [==============================] - 14s 196ms/step - loss: 0.1270 - accuracy: 0.9586 - val_loss: 1.8726 - val_accuracy: 0.2489\n",
            "Epoch 30/100\n",
            "66/66 [==============================] - 14s 196ms/step - loss: 0.1209 - accuracy: 0.9571 - val_loss: 2.1254 - val_accuracy: 0.2033\n",
            "Epoch 31/100\n",
            "66/66 [==============================] - 14s 196ms/step - loss: 0.1062 - accuracy: 0.9614 - val_loss: 1.5629 - val_accuracy: 0.4822\n",
            "Epoch 32/100\n",
            "66/66 [==============================] - 14s 198ms/step - loss: 0.1059 - accuracy: 0.9595 - val_loss: 1.4505 - val_accuracy: 0.3500\n",
            "Epoch 33/100\n",
            "66/66 [==============================] - 14s 197ms/step - loss: 0.1120 - accuracy: 0.9610 - val_loss: 2.6174 - val_accuracy: 0.2589\n",
            "Epoch 34/100\n",
            "66/66 [==============================] - 14s 196ms/step - loss: 0.1223 - accuracy: 0.9590 - val_loss: 1.7023 - val_accuracy: 0.3944\n",
            "Epoch 35/100\n",
            "66/66 [==============================] - 14s 197ms/step - loss: 0.0903 - accuracy: 0.9667 - val_loss: 1.1892 - val_accuracy: 0.4967\n",
            "Epoch 36/100\n",
            "66/66 [==============================] - 14s 197ms/step - loss: 0.1047 - accuracy: 0.9605 - val_loss: 1.7442 - val_accuracy: 0.3978\n",
            "Epoch 37/100\n",
            "66/66 [==============================] - 14s 196ms/step - loss: 0.0967 - accuracy: 0.9662 - val_loss: 1.3633 - val_accuracy: 0.4933\n",
            "Epoch 38/100\n",
            "66/66 [==============================] - 14s 194ms/step - loss: 0.0885 - accuracy: 0.9690 - val_loss: 1.2346 - val_accuracy: 0.5344\n",
            "Epoch 39/100\n",
            "66/66 [==============================] - 14s 197ms/step - loss: 0.0851 - accuracy: 0.9710 - val_loss: 1.4594 - val_accuracy: 0.3322\n",
            "Epoch 40/100\n",
            "66/66 [==============================] - 14s 195ms/step - loss: 0.0758 - accuracy: 0.9710 - val_loss: 2.3458 - val_accuracy: 0.2411\n",
            "Epoch 41/100\n",
            "66/66 [==============================] - 14s 195ms/step - loss: 0.0783 - accuracy: 0.9710 - val_loss: 1.3824 - val_accuracy: 0.4411\n",
            "Epoch 42/100\n",
            "66/66 [==============================] - 14s 198ms/step - loss: 0.1171 - accuracy: 0.9586 - val_loss: 1.4181 - val_accuracy: 0.3967\n",
            "Epoch 43/100\n",
            "66/66 [==============================] - 14s 195ms/step - loss: 0.1065 - accuracy: 0.9643 - val_loss: 1.0932 - val_accuracy: 0.5011\n",
            "Epoch 44/100\n",
            "66/66 [==============================] - 14s 195ms/step - loss: 0.0856 - accuracy: 0.9700 - val_loss: 1.6518 - val_accuracy: 0.3511\n",
            "Epoch 45/100\n",
            "66/66 [==============================] - 14s 195ms/step - loss: 0.0783 - accuracy: 0.9710 - val_loss: 1.2584 - val_accuracy: 0.4644\n",
            "Epoch 46/100\n",
            "66/66 [==============================] - 14s 196ms/step - loss: 0.0691 - accuracy: 0.9748 - val_loss: 1.2353 - val_accuracy: 0.4867\n",
            "Epoch 47/100\n",
            "66/66 [==============================] - 14s 194ms/step - loss: 0.0782 - accuracy: 0.9729 - val_loss: 1.2476 - val_accuracy: 0.4711\n",
            "Epoch 48/100\n",
            "66/66 [==============================] - 14s 197ms/step - loss: 0.0743 - accuracy: 0.9790 - val_loss: 1.2751 - val_accuracy: 0.4389\n",
            "Epoch 49/100\n",
            "66/66 [==============================] - 14s 193ms/step - loss: 0.0671 - accuracy: 0.9752 - val_loss: 1.5908 - val_accuracy: 0.3233\n",
            "Epoch 50/100\n",
            "66/66 [==============================] - 14s 194ms/step - loss: 0.1099 - accuracy: 0.9648 - val_loss: 2.2641 - val_accuracy: 0.2922\n",
            "Epoch 51/100\n",
            "66/66 [==============================] - 14s 195ms/step - loss: 0.0726 - accuracy: 0.9757 - val_loss: 1.3054 - val_accuracy: 0.4400\n",
            "Epoch 52/100\n",
            "66/66 [==============================] - 14s 195ms/step - loss: 0.0553 - accuracy: 0.9819 - val_loss: 1.0612 - val_accuracy: 0.5622\n",
            "Epoch 53/100\n",
            "66/66 [==============================] - 14s 196ms/step - loss: 0.0704 - accuracy: 0.9752 - val_loss: 1.5779 - val_accuracy: 0.4456\n",
            "Epoch 54/100\n",
            "66/66 [==============================] - 14s 195ms/step - loss: 0.0695 - accuracy: 0.9767 - val_loss: 1.7051 - val_accuracy: 0.4456\n",
            "Epoch 55/100\n",
            "66/66 [==============================] - 14s 195ms/step - loss: 0.0899 - accuracy: 0.9695 - val_loss: 1.4809 - val_accuracy: 0.4567\n",
            "Epoch 56/100\n",
            "66/66 [==============================] - 13s 193ms/step - loss: 0.0868 - accuracy: 0.9681 - val_loss: 2.2735 - val_accuracy: 0.1656\n",
            "Epoch 57/100\n",
            "66/66 [==============================] - 14s 197ms/step - loss: 0.1222 - accuracy: 0.9595 - val_loss: 1.8638 - val_accuracy: 0.2244\n",
            "Epoch 58/100\n",
            "66/66 [==============================] - 14s 195ms/step - loss: 0.0820 - accuracy: 0.9724 - val_loss: 1.1084 - val_accuracy: 0.5156\n",
            "Epoch 59/100\n",
            "66/66 [==============================] - 14s 196ms/step - loss: 0.0600 - accuracy: 0.9795 - val_loss: 1.9135 - val_accuracy: 0.2600\n",
            "Epoch 60/100\n",
            "66/66 [==============================] - 14s 197ms/step - loss: 0.0748 - accuracy: 0.9724 - val_loss: 1.7772 - val_accuracy: 0.2778\n",
            "Epoch 61/100\n",
            "66/66 [==============================] - 14s 197ms/step - loss: 0.0751 - accuracy: 0.9733 - val_loss: 1.4769 - val_accuracy: 0.4467\n",
            "Epoch 62/100\n",
            "66/66 [==============================] - 14s 194ms/step - loss: 0.0854 - accuracy: 0.9733 - val_loss: 2.2783 - val_accuracy: 0.2744\n",
            "Epoch 63/100\n",
            "66/66 [==============================] - 14s 195ms/step - loss: 0.0797 - accuracy: 0.9743 - val_loss: 1.2902 - val_accuracy: 0.4422\n",
            "Epoch 64/100\n",
            "66/66 [==============================] - 14s 195ms/step - loss: 0.0425 - accuracy: 0.9833 - val_loss: 1.4210 - val_accuracy: 0.4222\n",
            "Epoch 65/100\n",
            "66/66 [==============================] - 14s 194ms/step - loss: 0.0547 - accuracy: 0.9771 - val_loss: 1.0905 - val_accuracy: 0.5778\n",
            "Epoch 66/100\n",
            "66/66 [==============================] - 14s 195ms/step - loss: 0.0535 - accuracy: 0.9819 - val_loss: 1.6515 - val_accuracy: 0.4511\n",
            "Epoch 67/100\n",
            "66/66 [==============================] - 14s 197ms/step - loss: 0.0699 - accuracy: 0.9757 - val_loss: 1.1593 - val_accuracy: 0.5122\n",
            "Epoch 68/100\n",
            "66/66 [==============================] - 14s 195ms/step - loss: 0.1016 - accuracy: 0.9676 - val_loss: 1.1890 - val_accuracy: 0.5322\n",
            "Epoch 69/100\n",
            "66/66 [==============================] - 14s 194ms/step - loss: 0.0844 - accuracy: 0.9724 - val_loss: 1.0969 - val_accuracy: 0.5678\n",
            "Epoch 70/100\n",
            "66/66 [==============================] - 14s 194ms/step - loss: 0.0710 - accuracy: 0.9771 - val_loss: 2.8884 - val_accuracy: 0.1589\n",
            "Epoch 71/100\n",
            "66/66 [==============================] - 14s 196ms/step - loss: 0.0617 - accuracy: 0.9776 - val_loss: 1.5183 - val_accuracy: 0.4500\n",
            "Epoch 72/100\n",
            "66/66 [==============================] - 14s 195ms/step - loss: 0.0704 - accuracy: 0.9743 - val_loss: 0.9761 - val_accuracy: 0.6022\n",
            "Epoch 73/100\n",
            "66/66 [==============================] - 14s 195ms/step - loss: 0.0600 - accuracy: 0.9805 - val_loss: 1.0943 - val_accuracy: 0.5567\n",
            "Epoch 74/100\n",
            "66/66 [==============================] - 14s 196ms/step - loss: 0.0522 - accuracy: 0.9829 - val_loss: 1.3992 - val_accuracy: 0.4167\n",
            "Epoch 75/100\n",
            "66/66 [==============================] - 14s 195ms/step - loss: 0.0620 - accuracy: 0.9810 - val_loss: 1.0340 - val_accuracy: 0.5711\n",
            "Epoch 76/100\n",
            "66/66 [==============================] - 14s 193ms/step - loss: 0.0438 - accuracy: 0.9857 - val_loss: 1.4264 - val_accuracy: 0.4089\n",
            "Epoch 77/100\n",
            "66/66 [==============================] - 14s 194ms/step - loss: 0.0339 - accuracy: 0.9895 - val_loss: 1.8107 - val_accuracy: 0.2811\n",
            "Epoch 78/100\n",
            "66/66 [==============================] - 14s 193ms/step - loss: 0.0329 - accuracy: 0.9890 - val_loss: 1.6063 - val_accuracy: 0.3767\n",
            "Epoch 79/100\n",
            "66/66 [==============================] - 14s 193ms/step - loss: 0.0388 - accuracy: 0.9838 - val_loss: 1.7356 - val_accuracy: 0.4267\n",
            "Epoch 80/100\n",
            "66/66 [==============================] - 14s 195ms/step - loss: 0.1146 - accuracy: 0.9695 - val_loss: 2.4473 - val_accuracy: 0.3944\n",
            "Epoch 81/100\n",
            "66/66 [==============================] - 14s 194ms/step - loss: 0.1277 - accuracy: 0.9614 - val_loss: 1.1426 - val_accuracy: 0.4967\n",
            "Epoch 82/100\n",
            "66/66 [==============================] - 14s 195ms/step - loss: 0.0763 - accuracy: 0.9733 - val_loss: 1.7914 - val_accuracy: 0.3044\n",
            "Epoch 83/100\n",
            "66/66 [==============================] - 14s 197ms/step - loss: 0.0597 - accuracy: 0.9810 - val_loss: 1.0969 - val_accuracy: 0.5367\n",
            "Epoch 84/100\n",
            "66/66 [==============================] - 14s 196ms/step - loss: 0.0447 - accuracy: 0.9838 - val_loss: 1.9879 - val_accuracy: 0.2411\n",
            "Epoch 85/100\n",
            "66/66 [==============================] - 14s 192ms/step - loss: 0.0389 - accuracy: 0.9871 - val_loss: 1.3874 - val_accuracy: 0.4167\n",
            "Epoch 86/100\n",
            "66/66 [==============================] - 14s 195ms/step - loss: 0.0582 - accuracy: 0.9819 - val_loss: 1.6003 - val_accuracy: 0.3289\n",
            "Epoch 87/100\n",
            "66/66 [==============================] - 14s 196ms/step - loss: 0.0655 - accuracy: 0.9757 - val_loss: 0.9163 - val_accuracy: 0.6567\n",
            "Epoch 88/100\n",
            "66/66 [==============================] - 14s 195ms/step - loss: 0.0730 - accuracy: 0.9762 - val_loss: 0.8357 - val_accuracy: 0.6611\n",
            "Epoch 89/100\n",
            "66/66 [==============================] - 14s 195ms/step - loss: 0.0777 - accuracy: 0.9743 - val_loss: 0.8528 - val_accuracy: 0.6467\n",
            "Epoch 90/100\n",
            "66/66 [==============================] - 14s 197ms/step - loss: 0.0626 - accuracy: 0.9805 - val_loss: 1.8642 - val_accuracy: 0.2744\n",
            "Epoch 91/100\n",
            "66/66 [==============================] - 14s 195ms/step - loss: 0.0943 - accuracy: 0.9748 - val_loss: 1.4062 - val_accuracy: 0.4856\n",
            "Epoch 92/100\n",
            "66/66 [==============================] - 14s 196ms/step - loss: 0.0621 - accuracy: 0.9800 - val_loss: 1.2373 - val_accuracy: 0.4778\n",
            "Epoch 93/100\n",
            "66/66 [==============================] - 14s 194ms/step - loss: 0.0740 - accuracy: 0.9767 - val_loss: 0.7712 - val_accuracy: 0.6789\n",
            "Epoch 94/100\n",
            "66/66 [==============================] - 14s 194ms/step - loss: 0.0680 - accuracy: 0.9786 - val_loss: 1.2025 - val_accuracy: 0.5167\n",
            "Epoch 95/100\n",
            "66/66 [==============================] - 14s 197ms/step - loss: 0.0825 - accuracy: 0.9733 - val_loss: 1.7262 - val_accuracy: 0.3667\n",
            "Epoch 96/100\n",
            "66/66 [==============================] - 14s 194ms/step - loss: 0.0868 - accuracy: 0.9738 - val_loss: 1.8076 - val_accuracy: 0.4522\n",
            "Epoch 97/100\n",
            "66/66 [==============================] - 13s 192ms/step - loss: 0.0817 - accuracy: 0.9762 - val_loss: 2.0594 - val_accuracy: 0.3656\n",
            "Epoch 98/100\n",
            "66/66 [==============================] - 14s 194ms/step - loss: 0.0551 - accuracy: 0.9810 - val_loss: 1.8208 - val_accuracy: 0.3611\n",
            "Epoch 99/100\n",
            "66/66 [==============================] - 14s 195ms/step - loss: 0.0970 - accuracy: 0.9705 - val_loss: 1.7323 - val_accuracy: 0.4222\n",
            "Epoch 100/100\n",
            "66/66 [==============================] - 14s 194ms/step - loss: 0.0576 - accuracy: 0.9848 - val_loss: 1.5790 - val_accuracy: 0.3889\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history.history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMv3VqHmTaCJ",
        "outputId": "170e877e-2ef9-43d9-f432-442f60dc1e3c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': [0.7699999809265137,\n",
              "  0.8180952668190002,\n",
              "  0.8380952477455139,\n",
              "  0.8623809814453125,\n",
              "  0.8671428561210632,\n",
              "  0.883809506893158,\n",
              "  0.8904761672019958,\n",
              "  0.8947619199752808,\n",
              "  0.8966666460037231,\n",
              "  0.9095237851142883,\n",
              "  0.9176190495491028,\n",
              "  0.915238082408905,\n",
              "  0.9214285612106323,\n",
              "  0.9319047331809998,\n",
              "  0.9214285612106323,\n",
              "  0.9309523701667786,\n",
              "  0.9295238256454468,\n",
              "  0.9247618913650513,\n",
              "  0.9471428394317627,\n",
              "  0.938095211982727,\n",
              "  0.9528571367263794,\n",
              "  0.941428542137146,\n",
              "  0.9438095092773438,\n",
              "  0.9538094997406006,\n",
              "  0.95333331823349,\n",
              "  0.9480952620506287,\n",
              "  0.9576190710067749,\n",
              "  0.9566666483879089,\n",
              "  0.9585714340209961,\n",
              "  0.9571428298950195,\n",
              "  0.9614285826683044,\n",
              "  0.9595237970352173,\n",
              "  0.9609524011611938,\n",
              "  0.9590476155281067,\n",
              "  0.9666666388511658,\n",
              "  0.9604762196540833,\n",
              "  0.9661904573440552,\n",
              "  0.9690476059913635,\n",
              "  0.9709523916244507,\n",
              "  0.9709523916244507,\n",
              "  0.9709523916244507,\n",
              "  0.9585714340209961,\n",
              "  0.9642857313156128,\n",
              "  0.9700000286102295,\n",
              "  0.9709523916244507,\n",
              "  0.9747619032859802,\n",
              "  0.9728571176528931,\n",
              "  0.9790475964546204,\n",
              "  0.9752380847930908,\n",
              "  0.9647619128227234,\n",
              "  0.9757142663002014,\n",
              "  0.9819047451019287,\n",
              "  0.9752380847930908,\n",
              "  0.9766666889190674,\n",
              "  0.9695237874984741,\n",
              "  0.9680952429771423,\n",
              "  0.9595237970352173,\n",
              "  0.9723809361457825,\n",
              "  0.9795238375663757,\n",
              "  0.9723809361457825,\n",
              "  0.9733333587646484,\n",
              "  0.9733333587646484,\n",
              "  0.9742857217788696,\n",
              "  0.9833333492279053,\n",
              "  0.977142870426178,\n",
              "  0.9819047451019287,\n",
              "  0.9757142663002014,\n",
              "  0.9676190614700317,\n",
              "  0.9723809361457825,\n",
              "  0.977142870426178,\n",
              "  0.9776190519332886,\n",
              "  0.9742857217788696,\n",
              "  0.9804762005805969,\n",
              "  0.9828571677207947,\n",
              "  0.9809523820877075,\n",
              "  0.9857142567634583,\n",
              "  0.9895238280296326,\n",
              "  0.989047646522522,\n",
              "  0.9838095307350159,\n",
              "  0.9695237874984741,\n",
              "  0.9614285826683044,\n",
              "  0.9733333587646484,\n",
              "  0.9809523820877075,\n",
              "  0.9838095307350159,\n",
              "  0.9871428608894348,\n",
              "  0.9819047451019287,\n",
              "  0.9757142663002014,\n",
              "  0.976190447807312,\n",
              "  0.9742857217788696,\n",
              "  0.9804762005805969,\n",
              "  0.9747619032859802,\n",
              "  0.9800000190734863,\n",
              "  0.9766666889190674,\n",
              "  0.9785714149475098,\n",
              "  0.9733333587646484,\n",
              "  0.973809540271759,\n",
              "  0.976190447807312,\n",
              "  0.9809523820877075,\n",
              "  0.9704762101173401,\n",
              "  0.9847618937492371],\n",
              " 'loss': [0.5842495560646057,\n",
              "  0.4717694818973541,\n",
              "  0.4317571222782135,\n",
              "  0.3891870677471161,\n",
              "  0.3520647883415222,\n",
              "  0.2980731725692749,\n",
              "  0.3220255970954895,\n",
              "  0.2799093425273895,\n",
              "  0.26562461256980896,\n",
              "  0.2396695911884308,\n",
              "  0.21582596004009247,\n",
              "  0.23601940274238586,\n",
              "  0.20997324585914612,\n",
              "  0.18202243745326996,\n",
              "  0.22058241069316864,\n",
              "  0.18181559443473816,\n",
              "  0.18283379077911377,\n",
              "  0.19674566388130188,\n",
              "  0.1403411477804184,\n",
              "  0.15739092230796814,\n",
              "  0.13380396366119385,\n",
              "  0.15837249159812927,\n",
              "  0.15804529190063477,\n",
              "  0.11295069754123688,\n",
              "  0.12699438631534576,\n",
              "  0.14574983716011047,\n",
              "  0.11500166356563568,\n",
              "  0.11589251458644867,\n",
              "  0.1270146369934082,\n",
              "  0.12090950459241867,\n",
              "  0.10623160004615784,\n",
              "  0.10591337829828262,\n",
              "  0.11204513162374496,\n",
              "  0.12228803336620331,\n",
              "  0.09032455831766129,\n",
              "  0.10465947538614273,\n",
              "  0.09667809307575226,\n",
              "  0.08847669512033463,\n",
              "  0.08510107547044754,\n",
              "  0.07577931880950928,\n",
              "  0.07829850912094116,\n",
              "  0.11708027124404907,\n",
              "  0.10648003220558167,\n",
              "  0.08557431399822235,\n",
              "  0.07832317054271698,\n",
              "  0.0690528079867363,\n",
              "  0.07821120321750641,\n",
              "  0.07425686717033386,\n",
              "  0.06710362434387207,\n",
              "  0.10991480201482773,\n",
              "  0.07264405488967896,\n",
              "  0.055340491235256195,\n",
              "  0.07035177946090698,\n",
              "  0.06949735432863235,\n",
              "  0.08987069875001907,\n",
              "  0.0867546871304512,\n",
              "  0.12217581272125244,\n",
              "  0.08195867389440536,\n",
              "  0.05995415151119232,\n",
              "  0.07475479692220688,\n",
              "  0.07512445747852325,\n",
              "  0.08540159463882446,\n",
              "  0.07965583354234695,\n",
              "  0.0425308458507061,\n",
              "  0.054734278470277786,\n",
              "  0.053460005670785904,\n",
              "  0.06988167762756348,\n",
              "  0.10162366926670074,\n",
              "  0.08440278470516205,\n",
              "  0.0709800198674202,\n",
              "  0.06170383095741272,\n",
              "  0.07041598111391068,\n",
              "  0.060044433921575546,\n",
              "  0.05220149829983711,\n",
              "  0.062047649174928665,\n",
              "  0.04384501650929451,\n",
              "  0.03390046954154968,\n",
              "  0.032908618450164795,\n",
              "  0.03877145051956177,\n",
              "  0.11458582431077957,\n",
              "  0.1277064085006714,\n",
              "  0.07628487050533295,\n",
              "  0.05965996906161308,\n",
              "  0.04472853243350983,\n",
              "  0.03889451175928116,\n",
              "  0.05820564925670624,\n",
              "  0.06552005559206009,\n",
              "  0.07304063439369202,\n",
              "  0.0776870846748352,\n",
              "  0.0626458078622818,\n",
              "  0.09434046596288681,\n",
              "  0.06207664683461189,\n",
              "  0.07401376962661743,\n",
              "  0.06795110553503036,\n",
              "  0.08254057168960571,\n",
              "  0.08684147149324417,\n",
              "  0.081681989133358,\n",
              "  0.05513906851410866,\n",
              "  0.09701476246118546,\n",
              "  0.05756087973713875],\n",
              " 'val_accuracy': [0.3333333432674408,\n",
              "  0.31333333253860474,\n",
              "  0.25777778029441833,\n",
              "  0.33222222328186035,\n",
              "  0.3722222149372101,\n",
              "  0.29555556178092957,\n",
              "  0.2544444501399994,\n",
              "  0.28111112117767334,\n",
              "  0.3144444525241852,\n",
              "  0.23777778446674347,\n",
              "  0.4188888967037201,\n",
              "  0.30666667222976685,\n",
              "  0.2988888919353485,\n",
              "  0.2611111104488373,\n",
              "  0.18222221732139587,\n",
              "  0.4000000059604645,\n",
              "  0.30888888239860535,\n",
              "  0.4788888990879059,\n",
              "  0.4511111080646515,\n",
              "  0.3733333349227905,\n",
              "  0.19777777791023254,\n",
              "  0.17666666209697723,\n",
              "  0.46888887882232666,\n",
              "  0.425555557012558,\n",
              "  0.2222222238779068,\n",
              "  0.20555555820465088,\n",
              "  0.2666666805744171,\n",
              "  0.3733333349227905,\n",
              "  0.24888889491558075,\n",
              "  0.20333333313465118,\n",
              "  0.48222222924232483,\n",
              "  0.3499999940395355,\n",
              "  0.2588889002799988,\n",
              "  0.39444443583488464,\n",
              "  0.49666666984558105,\n",
              "  0.3977777659893036,\n",
              "  0.4933333396911621,\n",
              "  0.5344444513320923,\n",
              "  0.33222222328186035,\n",
              "  0.24111111462116241,\n",
              "  0.44111111760139465,\n",
              "  0.39666667580604553,\n",
              "  0.5011110901832581,\n",
              "  0.35111111402511597,\n",
              "  0.46444445848464966,\n",
              "  0.4866666793823242,\n",
              "  0.47111111879348755,\n",
              "  0.43888887763023376,\n",
              "  0.3233333230018616,\n",
              "  0.2922222316265106,\n",
              "  0.4399999976158142,\n",
              "  0.5622222423553467,\n",
              "  0.44555556774139404,\n",
              "  0.44555556774139404,\n",
              "  0.4566666781902313,\n",
              "  0.16555555164813995,\n",
              "  0.2244444489479065,\n",
              "  0.5155555605888367,\n",
              "  0.25999999046325684,\n",
              "  0.2777777910232544,\n",
              "  0.4466666579246521,\n",
              "  0.27444443106651306,\n",
              "  0.4422222077846527,\n",
              "  0.42222222685813904,\n",
              "  0.5777778029441833,\n",
              "  0.4511111080646515,\n",
              "  0.5122222304344177,\n",
              "  0.5322222113609314,\n",
              "  0.5677777528762817,\n",
              "  0.15888889133930206,\n",
              "  0.44999998807907104,\n",
              "  0.602222204208374,\n",
              "  0.5566666722297668,\n",
              "  0.4166666567325592,\n",
              "  0.5711110830307007,\n",
              "  0.40888887643814087,\n",
              "  0.28111112117767334,\n",
              "  0.3766666650772095,\n",
              "  0.4266666769981384,\n",
              "  0.39444443583488464,\n",
              "  0.49666666984558105,\n",
              "  0.30444443225860596,\n",
              "  0.5366666913032532,\n",
              "  0.24111111462116241,\n",
              "  0.4166666567325592,\n",
              "  0.3288888931274414,\n",
              "  0.6566666960716248,\n",
              "  0.6611111164093018,\n",
              "  0.6466666460037231,\n",
              "  0.27444443106651306,\n",
              "  0.4855555593967438,\n",
              "  0.47777777910232544,\n",
              "  0.6788889169692993,\n",
              "  0.5166666507720947,\n",
              "  0.36666667461395264,\n",
              "  0.45222222805023193,\n",
              "  0.3655555546283722,\n",
              "  0.3611111044883728,\n",
              "  0.42222222685813904,\n",
              "  0.3888888955116272],\n",
              " 'val_loss': [1.5117725133895874,\n",
              "  1.520485281944275,\n",
              "  1.5495145320892334,\n",
              "  1.4084049463272095,\n",
              "  1.583901047706604,\n",
              "  1.5439972877502441,\n",
              "  1.424966812133789,\n",
              "  1.6619864702224731,\n",
              "  1.5472201108932495,\n",
              "  1.6362813711166382,\n",
              "  1.2230132818222046,\n",
              "  1.9276355504989624,\n",
              "  1.7521069049835205,\n",
              "  1.5247913599014282,\n",
              "  1.8436871767044067,\n",
              "  1.2870720624923706,\n",
              "  1.352036714553833,\n",
              "  1.3819961547851562,\n",
              "  1.2065269947052002,\n",
              "  1.5979102849960327,\n",
              "  2.2531373500823975,\n",
              "  2.0443837642669678,\n",
              "  1.141594409942627,\n",
              "  1.4179877042770386,\n",
              "  1.7039309740066528,\n",
              "  2.277463912963867,\n",
              "  1.6800535917282104,\n",
              "  1.7045514583587646,\n",
              "  1.8725507259368896,\n",
              "  2.125410795211792,\n",
              "  1.5628774166107178,\n",
              "  1.45050048828125,\n",
              "  2.61738657951355,\n",
              "  1.702262282371521,\n",
              "  1.189182162284851,\n",
              "  1.7441502809524536,\n",
              "  1.3632514476776123,\n",
              "  1.2346487045288086,\n",
              "  1.4593747854232788,\n",
              "  2.345806837081909,\n",
              "  1.3824272155761719,\n",
              "  1.4180512428283691,\n",
              "  1.0931603908538818,\n",
              "  1.651766061782837,\n",
              "  1.258371353149414,\n",
              "  1.2353211641311646,\n",
              "  1.2476307153701782,\n",
              "  1.2750675678253174,\n",
              "  1.5908209085464478,\n",
              "  2.2640907764434814,\n",
              "  1.3053579330444336,\n",
              "  1.061168909072876,\n",
              "  1.5778752565383911,\n",
              "  1.7051290273666382,\n",
              "  1.4809097051620483,\n",
              "  2.2734715938568115,\n",
              "  1.8637560606002808,\n",
              "  1.1084067821502686,\n",
              "  1.913492202758789,\n",
              "  1.7772345542907715,\n",
              "  1.4768892526626587,\n",
              "  2.278294086456299,\n",
              "  1.2901641130447388,\n",
              "  1.421025037765503,\n",
              "  1.0904861688613892,\n",
              "  1.6514781713485718,\n",
              "  1.1592813730239868,\n",
              "  1.1890075206756592,\n",
              "  1.0969147682189941,\n",
              "  2.888382911682129,\n",
              "  1.5182878971099854,\n",
              "  0.9760658740997314,\n",
              "  1.094308853149414,\n",
              "  1.3992277383804321,\n",
              "  1.0339741706848145,\n",
              "  1.426405429840088,\n",
              "  1.810713529586792,\n",
              "  1.6062963008880615,\n",
              "  1.7356377840042114,\n",
              "  2.4472715854644775,\n",
              "  1.1425871849060059,\n",
              "  1.7914299964904785,\n",
              "  1.0968528985977173,\n",
              "  1.9878896474838257,\n",
              "  1.3873705863952637,\n",
              "  1.6002711057662964,\n",
              "  0.9162799119949341,\n",
              "  0.8357316255569458,\n",
              "  0.8527547717094421,\n",
              "  1.8641830682754517,\n",
              "  1.4061986207962036,\n",
              "  1.2372772693634033,\n",
              "  0.7712437510490417,\n",
              "  1.2024765014648438,\n",
              "  1.7262015342712402,\n",
              "  1.807624101638794,\n",
              "  2.059424877166748,\n",
              "  1.8208016157150269,\n",
              "  1.7322882413864136,\n",
              "  1.5790297985076904]}"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(os.getcwd() + '/small_CNN')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koXHTho5lGXA",
        "outputId": "ccd0f04e-3f27-4aae-f9b3-8fd0376f429d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/amlsAssignment/small_CNN/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "predictions = np.array([])\n",
        "labels =  np.array([])\n",
        "for x, y in validate_dataset:\n",
        "  predictions = np.concatenate([predictions, np.argmax(model.predict(x), axis = -1)])\n",
        "  labels = np.concatenate([labels, y.numpy()])"
      ],
      "metadata": {
        "id": "Ibu8ArzTlZnt"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "ConfusionMatrixDisplay.from_predictions(labels, predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "c-W4G9QVs3IJ",
        "outputId": "83653b10-94a1-41d2-a13e-e0abb80953f7"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fee4ef85b50>"
            ]
          },
          "metadata": {},
          "execution_count": 54
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}